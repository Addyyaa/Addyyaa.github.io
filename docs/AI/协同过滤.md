# 协同过滤

过拟合（Overfitting）是指模型在训练数据上表现得非常好，但在未见过的数据（测试集或验证集）上表现较差的现象。换句话说，模型过于“记住”了训练数据的细节和噪声，而没有学到数据的通用模式，导致其泛化能力不足。

![image-20250825222005488](./assets/image-20250825222005488.png)

**输出描述**：图中黑色点是带噪声的训练数据，蓝色实线是真实正弦函数，绿色虚线（3阶多项式）接近真实函数，红色虚线（15阶多项式）在训练点附近剧烈波动，试图“记住”每个点，这是过拟合的典型表现。：图中黑色点是带噪声的训练数据，蓝色实线是真实正弦函数，绿色虚线（3阶多项式）接近真实函数，红色虚线（15阶多项式）在训练点附近剧烈波动，试图“记住”每个点，这是过拟合的典型表现。

## 权重衰减

使用`L2`正则化来阻止**过拟合**，`L2`通过在损失函数中加入所有权重的平方和。这回给梯度增加一项从而促使权重尽可能变小。

原理：系数越大，损失函数的峡谷就越陡峭。如果使用基础的抛物线`y = a * (x**2)`例子举例，`a`越大，抛物线就越窄。

原文：

>Weight decay, or *L2 regularization*, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.
>
>Why would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, `y = a * (x**2)`, the larger `a` is, the more *narrow* the parabola is:

```python
x = np.linspace(-2,2,100)
a_s = [1,2,5,10,50] 
ys = [a * x**2 for a in a_s]
_,ax = plt.subplots(figsize=(8,6))
for a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')
ax.set_ylim([0,5])
ax.legend();
```

![image-20250825221644400](./assets/image-20250825221644400.png)

较大的参数会导致过于复杂的函数去拟合训练集中的所有数据点，这会引发过拟合。限制权重过度增长可能会对模型的训练造成一定阻碍，但会使模型的泛化能力增强。

原文：

>So, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.
>
>Limiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just `wd`) is a parameter that controls that sum of squares we add to our loss (assuming `parameters` is a tensor of all parameters):

```python
loss_with_wd = loss + wd * (parameters**2).sum()
```

公式：$loss\_with\_wd = loss + \lambda\sum_{i=1}^n\omega_i^2$

**也就是放大系数对损失函数的影响**

不过，在实际中这样操作，计算这么大的一个总和并将其加到损失函数中会非常低效（<span style="font-size:15px; font-family: '楷体', '楷体_GB2312', serif;"><strong>现代深度学习模型（如Transformer或大型神经网络）往往包含数百万到数十亿个参数（权重）。每次计算损失时，都需要遍历整个模型的所有权重，计算它们的平方并求和。这在训练循环中（涉及多次前向传播、反向传播和优化步骤）会反复执行，导致额外的计算负担，尤其在GPU/TPU等硬件上，内存访问和求和操作会消耗大量时间和资源。</strong></span>），而且可能在数值上不稳定。

高数中：$y=p^2$ 其导数 $y'=2p\frac{p}{dp}$

如上数学公式，所以把这个总和加到损失函数中，效果完全等同于：

```python
parameters.grad += wd * 2 * parameters
```

原文：

>In practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of `p**2` with respect to `p` is `2*p`, so adding that big sum to our loss is exactly the same as doing:

为什么等价？

**数学推导**

在梯度下降中，参数 $w_i$ 的更新基于总损失 $\text{loss\_with\_wd}$ 对 $w_i$ 的梯度：

>$\frac{\partial loss\_with\_wd}{\partial \omega_i}= \frac{\partial loss}{\partial \omega_i} + \frac{\partial}{\partial\omega_i}(\lambda\sum_j\omega_j^2)$

计算正则化项的导数：(<span style="color: red">反向传播时，通过对损失函数求导计算梯度来找到梯度度指向损失函数增加最快的方向，而梯度下降沿梯度的反方向更新参数以最小化损失”</span>)

>$\frac{\partial}{\partial w_i} \left( \lambda \sum_j w_j^2 \right) = \lambda \cdot \frac{\partial}{\partial w_i} \left( w_i^2 + \sum_{j \neq i} w_j^2 \right) = \lambda \cdot 2 w_i$

因此，总梯度为：

>$\frac{\partial \text{loss\_with\_wd}}{\partial w_i} = \frac{\partial \text{loss}}{\partial w_i} + 2 \lambda w_i$

对应代码：

```python
parameters.grad += wd * 2 * parameters
```

**梯度下降更新**

在梯度下降中，参数更新规则为：

>$w_i \gets w_i - \eta \cdot \frac{\partial \text{loss\_with\_wd}}{\partial w_i}$

代入梯度：

>$w_i \gets w_i - \eta \left( \frac{\partial \text{loss}}{\partial w_i} + 2 \lambda w_i \right)$

整理后：

>$w_i \gets w_i - \eta \frac{\partial \text{loss}}{\partial w_i} - \eta \cdot 2 \lambda w_i$

这里：

- 第一项 $-\eta \frac{\partial \text{loss}}{\partial w_i}$ 是基于原始损失的更新。
- 第二项 $-\eta \cdot 2 \lambda w_i$ 是一个与当前权重 $w_i$ 成正比的衰减项，效果是使权重向零靠拢。这正是权重衰减的实现方式。

**等价于权重衰减**

权重衰减的经典形式是在每次更新时直接让权重乘以一个小于 1 的因子，例如：
$$
w_i \gets (1 - \eta \cdot \lambda') w_i - \eta \cdot \frac{\partial \text{loss}}{\partial w_i}
$$
其中 $\lambda'$ 是权重衰减系数。比较 L2 正则化的更新：
$$
w_i \gets w_i - \eta \cdot 2 \lambda w_i - \eta \cdot \frac{\partial \text{loss}}{\partial w_i}
$$
可以看出，L2正则化的 $2 \lambda$ 对应权重衰减的 $\lambda'$，只是系数不同（$\lambda' \approx 2 \lambda$）。在实现中，框架（如`PyTorch`、`TensorFlow`）通常通过调整超参数（如 `wd`）来统一这两种形式。



在实际操作中，由于权重衰减是选择的一个参数，只需要将其设置为原来的两倍，这样在这个等式中甚至无需乘以2.要在`fastai`中使用权重衰减，只需要调用`fit`或`fit_one_cycle`函数时传入`wd`即可。

即把$$w_i \gets w_i - \eta \cdot 2 \lambda w_i - \eta \cdot \frac{\partial \text{loss}}{\partial w_i}$$中的$2\omega_i$使用$\omega'$代替

原文：

>In practice, since `wd` is a parameter that we choose, we can just make it twice as big, so we don't even need the `*2` in this equation. To use weight decay in fastai, just pass `wd` in your call to `fit` or `fit_one_cycle`:

```python
model = DotProductBias(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3, wd=0.1) # wd是权重衰减系数，人为指定
```

| epoch | train_loss | valid_loss | time  |
| :---: | :--------: | :--------: | :---: |
|   0   |  0.964149  |  0.947329  | 00:08 |
|   1   |  0.853209  |  0.862615  | 00:09 |
|   2   |  0.734107  |  0.828079  | 00:08 |
|   3   |  0.595621  |  0.812455  | 00:08 |
|   4   |  0.490830  |  0.812497  | 00:08 |